# v3.3.0 Comprehensive Test Specification

## Critical Gap Analysis

The current CSR-tester agent is missing critical test coverage for:

### 1. Hook System Testing
- **session-start hook**: Not tested at all
- **precompact hook**: Not tested at all
- **submit-hook**: Not tested
- Hook failure scenarios: Not covered
- Hook configuration validation: Missing

### 2. Sub-Agent Integration Testing
- **reflection-specialist**: Partial coverage
- **import-debugger**: NO coverage
- **docker-orchestrator**: NO coverage
- **mcp-integration**: NO coverage
- **search-optimizer**: NO coverage
- **qdrant-specialist**: NO coverage

### 3. Embedding Mode Testing
- **Zero vectors issue**: NOT investigated
- **Voyage AI full workflow**: Limited coverage
- **Mode switching**: Not properly tested
- **Collection naming consistency**: Partial
- **Dimension validation**: Limited

### 4. Import Pipeline Gaps
- **Streaming-watcher**: Limited testing
- **Delta-metadata updater**: Basic testing only
- **Concurrent imports**: Not tested
- **Resume from failure**: Not tested
- **Lock file handling**: Not tested

## Complete System Test Matrix

### A. Hook System Tests

```python
# test_hooks.py
import subprocess
import json
import os
from pathlib import Path

def test_session_start_hook():
    """Test session-start hook execution"""
    # Create test hook
    hook_content = """#!/bin/bash
echo "Session started at $(date)" >> ~/.claude-self-reflect/hooks.log
"""
    hook_path = Path.home() / '.claude' / 'hooks' / 'session-start'
    hook_path.parent.mkdir(parents=True, exist_ok=True)
    hook_path.write_text(hook_content)
    hook_path.chmod(0o755)

    # Trigger hook (simulate session start)
    # Verify execution
    log_path = Path.home() / '.claude-self-reflect' / 'hooks.log'
    assert log_path.exists(), "Hook did not execute"

def test_precompact_hook():
    """Test precompact hook for recent imports"""
    hook_script = """#!/bin/bash
source ~/projects/claude-self-reflect/venv/bin/activate
python ~/projects/claude-self-reflect/scripts/import-latest.py
"""
    # Test hook execution
    # Verify recent files imported

def test_hook_failure_handling():
    """Test system behavior when hooks fail"""
    # Create failing hook
    # Verify graceful handling
    # Check error logging
```

### B. Sub-Agent Validation Tests

```python
# test_subagents.py
import asyncio
from typing import Dict, Any

async def test_reflection_specialist():
    """Complete reflection-specialist agent test"""
    tests = {
        "search_past": "Find conversations about docker issues",
        "store_insight": "Remember this: zero vectors can occur from failed embeddings",
        "retrieve_full": "Get full conversation for cid-12345",
        "knowledge_continuity": "What did we discuss about imports yesterday?"
    }
    # Execute via Task tool
    # Verify responses

async def test_import_debugger():
    """Test import-debugger agent capabilities"""
    scenarios = [
        "JSONL shows 0 messages",
        "Import fails with dimension mismatch",
        "Chunking produces no output",
        "JQ filter returns empty"
    ]
    # Launch agent for each scenario
    # Verify diagnostic accuracy

async def test_qdrant_specialist():
    """Test qdrant-specialist agent"""
    tasks = [
        "Check collection health",
        "Diagnose search failures",
        "Fix dimension mismatches",
        "Optimize query performance"
    ]
    # Execute tasks
    # Verify solutions
```

### C. Zero Vectors Investigation

```python
# test_zero_vectors.py
import numpy as np
from qdrant_client import QdrantClient
from fastembed import TextEmbedding

def investigate_zero_vectors():
    """Root cause analysis for zero vector issue"""

    # Check 1: Embedding generation
    model = TextEmbedding("sentence-transformers/all-MiniLM-L6-v2")
    test_texts = ["test", "hello world", ""]

    for text in test_texts:
        embedding = list(model.embed([text]))[0]
        if np.all(embedding == 0):
            print(f"CRITICAL: Zero embedding for '{text}'")
        else:
            print(f"OK: Non-zero embedding for '{text}' (sum={np.sum(embedding):.2f})")

    # Check 2: Collection vectors
    client = QdrantClient("http://localhost:6333")
    collections = client.get_collections().collections

    for col in collections[:5]:
        points = client.scroll(
            collection_name=col.name,
            limit=10,
            with_vectors=True
        )[0]

        for point in points:
            if point.vector and all(v == 0 for v in point.vector):
                print(f"CRITICAL: Zero vector found in {col.name}, point {point.id}")

    # Check 3: Import pipeline
    # Trace through import-conversations-unified.py
    # Check embedding generation step
    # Verify vector before upsert

    return {
        "embedding_generation": "OK/FAIL",
        "stored_vectors": "OK/FAIL",
        "import_pipeline": "OK/FAIL"
    }
```

### D. Comprehensive Embedding Mode Tests

```python
# test_embedding_modes.py
import os
import asyncio
from pathlib import Path

async def test_local_mode_complete():
    """Test complete local mode workflow"""
    # Set environment
    os.environ['PREFER_LOCAL_EMBEDDINGS'] = 'true'

    # Test import
    subprocess.run([
        "python", "scripts/import-conversations-unified.py",
        "--limit", "5"
    ])

    # Test search
    # Test reflection storage
    # Verify 384 dimensions

async def test_voyage_mode_complete():
    """Test complete Voyage AI workflow"""
    # Set environment
    os.environ['PREFER_LOCAL_EMBEDDINGS'] = 'false'
    os.environ['VOYAGE_KEY'] = 'test-key'

    # Test import
    # Test search
    # Test reflection storage
    # Verify 1024 dimensions

async def test_mode_switching():
    """Test switching between modes"""
    # Start in local mode
    # Import some data
    # Switch to Voyage
    # Import more data
    # Verify both collections exist
    # Test cross-collection search
```

### E. Import Pipeline Stress Tests

```python
# test_import_stress.py
import concurrent.futures
import time

def test_concurrent_imports():
    """Test multiple simultaneous imports"""
    files = list(Path.home().glob(".claude/projects/**/*.jsonl"))[:10]

    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = []
        for file in files:
            future = executor.submit(import_file, file)
            futures.append(future)

        # Wait and check results
        results = [f.result() for f in futures]
        assert all(r['success'] for r in results)

def test_import_resume():
    """Test resume from failed import"""
    # Simulate partial import
    # Kill process mid-import
    # Restart import
    # Verify resume from checkpoint

def test_lock_file_handling():
    """Test import lock mechanisms"""
    # Create lock file
    # Attempt concurrent import
    # Verify proper waiting/timeout
```

### F. Watcher System Tests

```python
# test_watchers.py
import docker
import time

def test_streaming_watcher():
    """Test streaming watcher functionality"""
    client = docker.from_env()

    # Check container running
    containers = client.containers.list(filters={"name": "watcher"})
    assert len(containers) > 0, "Watcher not running"

    # Create new JSONL file
    test_file = Path.home() / ".claude/projects/test-project/test.jsonl"
    test_file.parent.mkdir(parents=True, exist_ok=True)
    test_file.write_text('{"messages": [], "timestamp": "2025-01-01T00:00:00Z"}')

    # Wait for processing
    time.sleep(5)

    # Verify imported
    state_file = Path.home() / ".claude-self-reflect/config/csr-watcher.json"
    state = json.loads(state_file.read_text())
    assert str(test_file) in state.get("imported_files", {})

def test_delta_metadata_updater():
    """Test metadata extraction and update"""
    # Run delta update
    result = subprocess.run([
        "python", "scripts/delta-metadata-update.py",
        "--dry-run"
    ], capture_output=True, text=True)

    # Verify metadata extracted
    assert "files_analyzed" in result.stdout
    assert "tools_used" in result.stdout
    assert "concepts" in result.stdout
```

### G. MCP Integration Tests

```python
# test_mcp_integration.py
async def test_all_mcp_tools():
    """Test all 15+ MCP tools comprehensively"""

    tools = [
        # Search tools
        ("reflect_on_past", {"query": "test", "limit": 3}),
        ("quick_search", {"query": "docker"}),
        ("search_summary", {"query": "imports"}),
        ("search_by_file", {"file_path": "server.py"}),
        ("search_by_concept", {"concept": "testing"}),

        # Temporal tools
        ("get_recent_work", {"limit": 5}),
        ("search_by_recency", {"query": "bug", "time_range": "last week"}),
        ("get_timeline", {"time_range": "last month"}),

        # Reflection tools
        ("store_reflection", {"content": "Test", "tags": ["test"]}),
        ("get_full_conversation", {"conversation_id": "test-id"}),

        # Pagination
        ("get_more_results", {"query": "test", "offset": 3}),
        ("get_next_results", {"query": "test", "offset": 3})
    ]

    for tool_name, params in tools:
        try:
            result = await call_mcp_tool(tool_name, params)
            assert result is not None, f"{tool_name} returned None"
            assert "error" not in str(result).lower(), f"{tool_name} returned error"
        except Exception as e:
            print(f"FAIL: {tool_name} - {e}")
```

## Test Execution Order

### Phase 1: Foundation Tests
1. System health check
2. Docker services validation
3. Qdrant collections check
4. MCP connection test

### Phase 2: Critical Issue Tests
1. Zero vectors investigation
2. Dimension mismatch testing
3. XML injection validation
4. Import data loss scenarios

### Phase 3: Component Tests
1. Hook system validation
2. Import pipeline tests
3. Watcher functionality
4. Delta metadata updater

### Phase 4: Integration Tests
1. All MCP tools
2. Sub-agent coordination
3. Mode switching
4. Cross-collection search

### Phase 5: Stress Tests
1. Concurrent operations
2. Large file imports
3. Memory pressure scenarios
4. Performance benchmarks

### Phase 6: End-to-End Workflows
1. Complete local mode workflow
2. Complete Voyage mode workflow
3. Mixed mode operations
4. Full system recovery test

## Test Automation Script

```bash
#!/bin/bash
# run_comprehensive_tests.sh

echo "=== V3.3.0 COMPREHENSIVE TEST SUITE ==="
echo "Starting: $(date)"

# Create results directory
RESULTS_DIR="test-results-$(date +%Y%m%d-%H%M%S)"
mkdir -p "$RESULTS_DIR"

# Run test phases
echo "Phase 1: Foundation Tests..."
python tests/test_foundation.py > "$RESULTS_DIR/foundation.log" 2>&1

echo "Phase 2: Critical Issues..."
python tests/test_critical_issues.py > "$RESULTS_DIR/critical.log" 2>&1

echo "Phase 3: Components..."
python tests/test_components.py > "$RESULTS_DIR/components.log" 2>&1

echo "Phase 4: Integration..."
python tests/test_integration.py > "$RESULTS_DIR/integration.log" 2>&1

echo "Phase 5: Stress Tests..."
python tests/test_stress.py > "$RESULTS_DIR/stress.log" 2>&1

echo "Phase 6: End-to-End..."
python tests/test_e2e.py > "$RESULTS_DIR/e2e.log" 2>&1

# Generate report
python tests/generate_report.py "$RESULTS_DIR" > "$RESULTS_DIR/report.md"

echo "Complete: $(date)"
echo "Results: $RESULTS_DIR/report.md"
```

## Success Criteria

### Critical (Must Pass)
- [ ] Zero vectors issue resolved
- [ ] All 15+ MCP tools functional
- [ ] Both embedding modes working
- [ ] No data loss during import
- [ ] Hooks execute properly
- [ ] Docker services healthy

### Important (Should Pass)
- [ ] All sub-agents functional
- [ ] Mode switching works
- [ ] Concurrent imports succeed
- [ ] Performance < 2s for search
- [ ] Memory < 1GB usage
- [ ] Metadata extraction works

### Nice to Have
- [ ] 100% test coverage
- [ ] Sub-second responses
- [ ] Zero errors in logs

## Certification Requirements

Before declaring v3.3.0 production ready:

1. **All critical tests must pass**
2. **GPT-5 review of all fixes applied**
3. **Both embedding modes fully tested**
4. **System restored to local mode**
5. **No critical or high priority issues remain**
6. **Comprehensive test report generated**
7. **Sign-off from CSR-tester agent**

This specification addresses ALL gaps identified in the current testing coverage and ensures comprehensive validation of the entire Claude Self-Reflect system.